# -*- coding: utf-8 -*-
"""MLmodelPMGT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bi4uFX72_Roq4fQtsfwV4arzY2evBpux
"""

# Re-import the necessary library
import pandas as pd

# Load the new dataset provided by the user
new_file_path = 'cleaned_tasks.csv'
new_tasks_df = pd.read_csv(new_file_path)

# Display the first few rows to analyze the new dataset structure
new_tasks_df.head()

import matplotlib.pyplot as plt

# Convert dates to datetime format for better visualization
new_tasks_df['Creation Date'] = pd.to_datetime(new_tasks_df['Creation Date'], format='%d/%m/%Y')
new_tasks_df['Deadline'] = pd.to_datetime(new_tasks_df['Deadline'], format='%d/%m/%Y')

# Visualization 1: Distribution of Priority Levels
plt.figure(figsize=(10, 6))
new_tasks_df['Priority Level'].value_counts().plot(kind='bar', color='skyblue')
plt.title('Distribution of Priority Levels')
plt.xlabel('Priority Level')
plt.ylabel('Number of Tasks')
plt.xticks(rotation=45)
plt.show()

# Visualization 2: Task Status vs. Business Impact
plt.figure(figsize=(10, 6))
impact_status_counts = new_tasks_df.groupby(['Current Status', 'Business Impact']).size().unstack()
impact_status_counts.plot(kind='bar', stacked=True, colormap='viridis')
plt.title('Task Status vs. Business Impact')
plt.xlabel('Task Status')
plt.ylabel('Number of Tasks')
plt.xticks(rotation=45)
plt.show()

# Visualization 3: Estimated Effort by Priority Level
plt.figure(figsize=(10, 6))
new_tasks_df.boxplot(column='Estimated Effort (Hours)', by='Priority Level', grid=False, patch_artist=True)
plt.title('Estimated Effort by Priority Level')
plt.suptitle('')
plt.xlabel('Priority Level')
plt.ylabel('Estimated Effort (Hours)')
plt.show()

from sklearn.preprocessing import LabelEncoder

# Step 1: Handle Date Features
# Create a new feature: 'Days Until Deadline'
new_tasks_df['Days Until Deadline'] = (new_tasks_df['Deadline'] - new_tasks_df['Creation Date']).dt.days

# Step 2: Encode Categorical Variables
label_encoders = {}
for column in ['Task Type', 'Current Status', 'Assignee Name', 'Business Impact']:
    le = LabelEncoder()
    new_tasks_df[column] = le.fit_transform(new_tasks_df[column])
    label_encoders[column] = le

# Encode 'Priority Level' which will be our target variable
priority_le = LabelEncoder()
new_tasks_df['Priority Level Encoded'] = priority_le.fit_transform(new_tasks_df['Priority Level'])

# Step 3: Handle Missing Values (if any)
missing_values = new_tasks_df.isnull().sum()

# Step 4: Scaling (optional, depending on the model choice, not applied here)
# We will leave this step out for now as RandomForest does not require feature scaling.

# Display missing values and first few rows of the preprocessed dataset
missing_values, new_tasks_df.head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Step 1: Split the data into training and testing sets
X = new_tasks_df[['Task Type', 'Current Status', 'Estimated Effort (Hours)', 'Business Impact', 'Days Until Deadline']]
y = new_tasks_df['Priority Level Encoded']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Train a Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

# Step 3: Make predictions and evaluate the model
y_pred = rf_classifier.predict(X_test)

# Generate classification report
report = classification_report(y_test, y_pred, target_names=priority_le.classes_)

# Display the classification report
report

"""The Random Forest model has been trained and evaluated. Here are the key results:

* Overall Accuracy: 33%
* Precision & Recall:
 * High Priority: Precision is 31%, recall is 18%.
 * Low Priority: Precision is 16%, recall is 20%.
 * Medium Priority: Precision is 46%, recall is 57%.
**Observations:**
* The model performs best in predicting "Medium" priority tasks but struggles with "High" and "Low" priorities.
* This suggests that the model may benefit from further tuning or additional data to improve its ability to distinguish between different priority levels.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'class_weight': ['balanced', None]
}

# Initialize the RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters from grid search
best_rf = grid_search.best_estimator_

# Make predictions on the test set using the best model
y_pred_best = best_rf.predict(X_test)

# Generate classification report
improved_report = classification_report(y_test, y_pred_best, target_names=priority_le.classes_)

# Display the best hyperparameters and the improved classification report
best_params = grid_search.best_params_
improved_report, best_params

from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import classification_report

# Initialize the Gradient Boosting model
gb_classifier = GradientBoostingClassifier(random_state=42)

# Combine RandomForest and GradientBoosting in a Voting Classifier
voting_classifier = VotingClassifier(estimators=[
    ('rf', best_rf),
    ('gb', gb_classifier)
], voting='soft', n_jobs=-1)

# Train the Voting Classifier
voting_classifier.fit(X_train, y_train)

# Make predictions with the Voting Classifier
y_pred_voting = voting_classifier.predict(X_test)

# Generate a classification report
voting_report = classification_report(y_test, y_pred_voting, target_names=priority_le.classes_)

# Display the classification report for the Voting Classifier
voting_report

from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Step 1: Feature Engineering - Adding Interaction Term between 'Business Impact' and 'Estimated Effort (Hours)'
new_tasks_df['Impact_Effort'] = new_tasks_df['Business Impact'] * new_tasks_df['Estimated Effort (Hours)']

# Step 2: Log Transformation of 'Estimated Effort (Hours)' to reduce skewness
new_tasks_df['Log_Estimated_Effort'] = np.log1p(new_tasks_df['Estimated Effort (Hours)'])

# Select the updated features including new engineered features
X_new = new_tasks_df[['Task Type', 'Current Status', 'Business Impact', 'Days Until Deadline', 'Impact_Effort', 'Log_Estimated_Effort']]
y_new = new_tasks_df['Priority Level Encoded']

# Split the data into training and testing sets
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)

# Train the Random Forest classifier with the new features
best_rf.fit(X_train_new, y_train_new)

# Train the Gradient Boosting model with the new features
gb_classifier.fit(X_train_new, y_train_new)

# Combine RandomForest and GradientBoosting in a Voting Classifier
voting_classifier_new = VotingClassifier(estimators=[
    ('rf', best_rf),
    ('gb', gb_classifier)
], voting='soft', n_jobs=-1)

# Train the Voting Classifier
voting_classifier_new.fit(X_train_new, y_train_new)

# Make predictions with the Voting Classifier
y_pred_voting_new = voting_classifier_new.predict(X_test_new)

# Generate a classification report
voting_report_new = classification_report(y_test_new, y_pred_voting_new, target_names=priority_le.classes_)

# Display the classification report for the updated Voting Classifier
voting_report_new

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Initialize XGBoost model
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')

# Initialize LightGBM model
lgb_model = LGBMClassifier(random_state=42)

# Train XGBoost model
xgb_model.fit(X_train_new, y_train_new)

# Train LightGBM model
lgb_model.fit(X_train_new, y_train_new)

# Make predictions with both models
y_pred_xgb = xgb_model.predict(X_test_new)
y_pred_lgb = lgb_model.predict(X_test_new)

# Generate classification reports
xgb_report = classification_report(y_test_new, y_pred_xgb, target_names=priority_le.classes_)
lgb_report = classification_report(y_test_new, y_pred_lgb, target_names=priority_le.classes_)

xgb_report, lgb_report

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0]
}

# Define the parameter grid for LightGBM
lgb_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0]
}

# Initialize the GridSearchCV for XGBoost
xgb_grid_search = GridSearchCV(estimator=XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),
                               param_grid=xgb_param_grid, cv=3, n_jobs=-1, verbose=2)

# Initialize the GridSearchCV for LightGBM
lgb_grid_search = GridSearchCV(estimator=LGBMClassifier(random_state=42),
                               param_grid=lgb_param_grid, cv=3, n_jobs=-1, verbose=2)

# Perform the grid search for XGBoost
xgb_grid_search.fit(X_train_new, y_train_new)

# Perform the grid search for LightGBM
lgb_grid_search.fit(X_train_new, y_train_new)

# Get the best models from the grid search
best_xgb_model = xgb_grid_search.best_estimator_
best_lgb_model = lgb_grid_search.best_estimator_

# Display the best parameters found
xgb_best_params = xgb_grid_search.best_params_
lgb_best_params = lgb_grid_search.best_params_

xgb_best_params, lgb_best_params

# Re-import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# Load the dataset again
file_path = 'cleaned_tasks.csv'
new_tasks_df = pd.read_csv(file_path)

# Convert dates to datetime format
new_tasks_df['Creation Date'] = pd.to_datetime(new_tasks_df['Creation Date'], format='%d/%m/%Y')
new_tasks_df['Deadline'] = pd.to_datetime(new_tasks_df['Deadline'], format='%d/%m/%Y')

# Create a new feature: 'Days Until Deadline'
new_tasks_df['Days Until Deadline'] = (new_tasks_df['Deadline'] - new_tasks_df['Creation Date']).dt.days

# Encode categorical variables
label_encoders = {}
for column in ['Task Type', 'Current Status', 'Assignee Name', 'Business Impact']:
    le = LabelEncoder()
    new_tasks_df[column] = le.fit_transform(new_tasks_df[column])
    label_encoders[column] = le

# Encode 'Priority Level' which will be our target variable
priority_le = LabelEncoder()
new_tasks_df['Priority Level Encoded'] = priority_le.fit_transform(new_tasks_df['Priority Level'])

# Feature Engineering
# Step 1: Adding Interaction Term between 'Business Impact' and 'Estimated Effort (Hours)'
new_tasks_df['Impact_Effort'] = new_tasks_df['Business Impact'] * new_tasks_df['Estimated Effort (Hours)']

# Step 2: Log Transformation of 'Estimated Effort (Hours)' to reduce skewness
new_tasks_df['Log_Estimated_Effort'] = np.log1p(new_tasks_df['Estimated Effort (Hours)'])

# Select the updated features including new engineered features
X_new = new_tasks_df[['Task Type', 'Current Status', 'Business Impact', 'Days Until Deadline', 'Impact_Effort', 'Log_Estimated_Effort']]
y_new = new_tasks_df['Priority Level Encoded']

# Split the data into training and testing sets
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)

# Best hyperparameters from the previous step
xgb_best_params = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}
lgb_best_params = {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}

# Define the base models with the best hyperparameters
xgb_best = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='mlogloss')
lgb_best = LGBMClassifier(**lgb_best_params, random_state=42)

# Define the stacking ensemble with a Logistic Regression meta-model
stacking_model = StackingClassifier(
    estimators=[('xgb', xgb_best), ('lgb', lgb_best)],
    final_estimator=LogisticRegression(),
    cv=3,
    n_jobs=-1
)

# Train the stacking model
stacking_model.fit(X_train_new, y_train_new)

# Make predictions with the stacking model
y_pred_stacking = stacking_model.predict(X_test_new)

# Generate and display the classification report for the stacking model
stacking_report = classification_report(y_test_new, y_pred_stacking, target_names=priority_le.classes_)
stacking_report

"""**Observations:**
* The model is heavily biased towards predicting "Medium" priority tasks, while failing to predict "High" priority tasks entirely.
* The absence of predictions for the "High" priority class suggests that the model might require better balance in terms of class representation or further refinement in hyperparameters.
"""

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_new, y_train_new)

# Define the RandomForest model with class weighting as a baseline for comparison
rf_balanced = RandomForestClassifier(class_weight='balanced', random_state=42)

# Train the RandomForest model on the balanced dataset
rf_balanced.fit(X_train_balanced, y_train_balanced)

# Make predictions on the test set
y_pred_balanced = rf_balanced.predict(X_test_new)

# Generate and display the classification report for the balanced model
balanced_report = classification_report(y_test_new, y_pred_balanced, target_names=priority_le.classes_)
balanced_report

# Re-import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# Load the dataset again
file_path = 'cleaned_tasks.csv'
new_tasks_df = pd.read_csv(file_path)

# Convert dates to datetime format
new_tasks_df['Creation Date'] = pd.to_datetime(new_tasks_df['Creation Date'], format='%d/%m/%Y')
new_tasks_df['Deadline'] = pd.to_datetime(new_tasks_df['Deadline'], format='%d/%m/%Y')

# Create a new feature: 'Days Until Deadline'
new_tasks_df['Days Until Deadline'] = (new_tasks_df['Deadline'] - new_tasks_df['Creation Date']).dt.days

# Encode categorical variables
label_encoders = {}
for column in ['Task Type', 'Current Status', 'Assignee Name', 'Business Impact']:
    le = LabelEncoder()
    new_tasks_df[column] = le.fit_transform(new_tasks_df[column])
    label_encoders[column] = le

# Encode 'Priority Level' which will be our target variable
priority_le = LabelEncoder()
new_tasks_df['Priority Level Encoded'] = priority_le.fit_transform(new_tasks_df['Priority Level'])

# Feature Engineering
# Step 1: Adding Interaction Term between 'Business Impact' and 'Estimated Effort (Hours)'
new_tasks_df['Impact_Effort'] = new_tasks_df['Business Impact'] * new_tasks_df['Estimated Effort (Hours)']

# Step 2: Log Transformation of 'Estimated Effort (Hours)' to reduce skewness
new_tasks_df['Log_Estimated_Effort'] = np.log1p(new_tasks_df['Estimated Effort (Hours)'])

# Select the updated features including new engineered features
X_new = new_tasks_df[['Task Type', 'Current Status', 'Business Impact', 'Days Until Deadline', 'Impact_Effort', 'Log_Estimated_Effort']]
y_new = new_tasks_df['Priority Level Encoded']

# Split the data into training and testing sets
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)

# Define the RandomForest model with class weighting
rf_class_weighted = RandomForestClassifier(class_weight='balanced', random_state=42)

# Train the RandomForest model with class weighting
rf_class_weighted.fit(X_train_new, y_train_new)

# Make predictions on the test set
y_pred_class_weighted = rf_class_weighted.predict(X_test_new)

# Generate and display the classification report for the class-weighted model
class_weighted_report = classification_report(y_test_new, y_pred_class_weighted, target_names=priority_le.classes_)
class_weighted_report

"""**Interpretation:**
* The model is making more balanced predictions across the different priority levels compared to earlier iterations.
* High Priority tasks have the best precision, but recall is relatively lower, indicating that while correct predictions are accurate, not all high-priority tasks are being identified.
* Low Priority tasks show lower precision, suggesting the model still struggles with false positives in this category.
"""

# Define the parameter grid for further tuning
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced', 'balanced_subsample']
}

# Initialize the RandomForestClassifier
rf_tuning = RandomForestClassifier(random_state=42)

# Perform a Grid Search with Cross-Validation
grid_search_rf = GridSearchCV(estimator=rf_tuning, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)
grid_search_rf.fit(X_train_new, y_train_new)

# Get the best model from the grid search
best_rf_tuned = grid_search_rf.best_estimator_

# Make predictions on the test set using the best model
y_pred_rf_tuned = best_rf_tuned.predict(X_test_new)

# Generate the classification report
tuned_report_rf = classification_report(y_test_new, y_pred_rf_tuned, target_names=priority_le.classes_)

# Display the best hyperparameters and the classification report
best_rf_params = grid_search_rf.best_params_
tuned_report_rf, best_rf_params

# Recreate the engineered features
new_tasks_df['Impact_Effort'] = new_tasks_df['Business Impact'] * new_tasks_df['Estimated Effort (Hours)']
new_tasks_df['Log_Estimated_Effort'] = np.log1p(new_tasks_df['Estimated Effort (Hours)'])

# Now select the features and target variable
X_new = new_tasks_df[['Task Type', 'Current Status', 'Business Impact', 'Days Until Deadline', 'Impact_Effort', 'Log_Estimated_Effort']]
y_new = new_tasks_df['Priority Level Encoded']

# Split the data into training and testing sets
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)

# Re-create and train the RandomForest model with the best hyperparameters
best_rf_tuned = RandomForestClassifier(
    n_estimators=300,
    max_depth=10,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight='balanced',
    random_state=42
)

best_rf_tuned.fit(X_train_new, y_train_new)

# Save the finalized model to a file
model_filename = 'final_rf_model_corrected.pkl'
joblib.dump(best_rf_tuned, model_filename)

# Output the path to the saved model file
model_filename